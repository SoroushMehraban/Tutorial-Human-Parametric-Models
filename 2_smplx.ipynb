{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a49789d",
   "metadata": {},
   "source": [
    "# SMPL-X: Expressive Body Capture\n",
    "Download the model files from [here](https://smpl-x.is.tue.mpg.de) and place them in the `parametric_models` folder.\n",
    "After logging in, go to the \"Download\" tab and \"Download SMPL-X v1.1 (NPZ+PKL, 830 MB)\".\n",
    "\n",
    "The packages required for this is same as before in addition to these packages:\n",
    "```bash\n",
    "tqdm\n",
    "imageio[ffmpeg]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cbc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smplx import SMPLX\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "import pyrender\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f2bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMPLX_MODEL_PATH = \"./parametric_models/smplx\"\n",
    "\n",
    "smplx_model = SMPLX(\n",
    "    model_path=SMPLX_MODEL_PATH,\n",
    "    gender='neutral',\n",
    "    use_pca=False,\n",
    "    num_betas=10,\n",
    "    num_expression_coeffs=100,\n",
    "    flat_hand_mean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e97db",
   "metadata": {},
   "source": [
    "### Using the model with a real mocap data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d18b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poses',\n",
       " 'trans',\n",
       " 'betas',\n",
       " 'expressions',\n",
       " 'model',\n",
       " 'gender',\n",
       " 'mocap_frame_rate']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('sample_smplx.npz')\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f2efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas shape: (10,)\n",
      "Poses shape: (1016, 165)\n",
      "Expressions shape: (1016, 100)\n",
      "Translation shape: (1016, 3)\n"
     ]
    }
   ],
   "source": [
    "betas = data['betas']\n",
    "poses = data['poses']\n",
    "expressions = data['expressions']\n",
    "transl = data['trans']\n",
    "\n",
    "print(\"Betas shape:\", betas.shape)\n",
    "print(\"Poses shape:\", poses.shape)\n",
    "print(\"Expressions shape:\", expressions.shape)\n",
    "print(\"Translation shape:\", transl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c923a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict = {\n",
    "    0: 'pelvis',\n",
    "    1: 'left_hip',\n",
    "    2: 'right_hip',\n",
    "    3: 'spine1',\n",
    "    4: 'left_knee',\n",
    "    5: 'right_knee',\n",
    "    6: 'spine2',\n",
    "    7: 'left_ankle',\n",
    "    8: 'right_ankle',\n",
    "    9: 'spine3',\n",
    "    10: 'left_foot',\n",
    "    11: 'right_foot',\n",
    "    12: 'neck',\n",
    "    13: 'left_collar',\n",
    "    14: 'right_collar',\n",
    "    15: 'head',\n",
    "    16: 'left_shoulder',\n",
    "    17: 'right_shoulder',\n",
    "    18: 'left_elbow',\n",
    "    19: 'right_elbow',\n",
    "    20: 'left_wrist',\n",
    "    21: 'right_wrist',\n",
    "    22: 'jaw',\n",
    "    23: 'left_eye_smplhf',\n",
    "    24: 'right_eye_smplhf',\n",
    "    25: 'left_index1',\n",
    "    26: 'left_index2',\n",
    "    27: 'left_index3',\n",
    "    28: 'left_middle1',\n",
    "    29: 'left_middle2',\n",
    "    30: 'left_middle3',\n",
    "    31: 'left_pinky1',\n",
    "    32: 'left_pinky2',\n",
    "    33: 'left_pinky3',\n",
    "    34: 'left_ring1',\n",
    "    35: 'left_ring2',\n",
    "    36: 'left_ring3',\n",
    "    37: 'left_thumb1',\n",
    "    38: 'left_thumb2',\n",
    "    39: 'left_thumb3',\n",
    "    40: 'right_index1',\n",
    "    41: 'right_index2',\n",
    "    42: 'right_index3',\n",
    "    43: 'right_middle1',\n",
    "    44: 'right_middle2',\n",
    "    45: 'right_middle3',\n",
    "    46: 'right_pinky1',\n",
    "    47: 'right_pinky2',\n",
    "    48: 'right_pinky3',\n",
    "    49: 'right_ring1',\n",
    "    50: 'right_ring2',\n",
    "    51: 'right_ring3',\n",
    "    52: 'right_thumb1',\n",
    "    53: 'right_thumb2',\n",
    "    54: 'right_thumb3',\n",
    "    55: 'nose',\n",
    "    56: 'right_eye',\n",
    "    57: 'left_eye',\n",
    "    58: 'right_ear',\n",
    "    59: 'left_ear',\n",
    "    60: 'left_big_toe',\n",
    "    61: 'left_small_toe',\n",
    "    62: 'left_heel',\n",
    "    63: 'right_big_toe',\n",
    "    64: 'right_small_toe',\n",
    "    65: 'right_heel',\n",
    "    66: 'left_thumb',\n",
    "    67: 'left_index',\n",
    "    68: 'left_middle',\n",
    "    69: 'left_ring',\n",
    "    70: 'left_pinky',\n",
    "    71: 'right_thumb',\n",
    "    72: 'right_index',\n",
    "    73: 'right_middle',\n",
    "    74: 'right_ring',\n",
    "    75: 'right_pinky',\n",
    "    76: 'right_eye_brow1',\n",
    "    77: 'right_eye_brow2',\n",
    "    78: 'right_eye_brow3',\n",
    "    79: 'right_eye_brow4',\n",
    "    80: 'right_eye_brow5',\n",
    "    81: 'left_eye_brow5',\n",
    "    82: 'left_eye_brow4',\n",
    "    83: 'left_eye_brow3',\n",
    "    84: 'left_eye_brow2',\n",
    "    85: 'left_eye_brow1',\n",
    "    86: 'nose1',\n",
    "    87: 'nose2',\n",
    "    88: 'nose3',\n",
    "    89: 'nose4',\n",
    "    90: 'right_nose_2',\n",
    "    91: 'right_nose_1',\n",
    "    92: 'nose_middle',\n",
    "    93: 'left_nose_1',\n",
    "    94: 'left_nose_2',\n",
    "    95: 'right_eye1',\n",
    "    96: 'right_eye2',\n",
    "    97: 'right_eye3',\n",
    "    98: 'right_eye4',\n",
    "    99: 'right_eye5',\n",
    "    100: 'right_eye6',\n",
    "    101: 'left_eye4',\n",
    "    102: 'left_eye3',\n",
    "    103: 'left_eye2',\n",
    "    104: 'left_eye1',\n",
    "    105: 'left_eye6',\n",
    "    106: 'left_eye5',\n",
    "    107: 'right_mouth_1',\n",
    "    108: 'right_mouth_2',\n",
    "    109: 'right_mouth_3',\n",
    "    110: 'mouth_top',\n",
    "    111: 'left_mouth_3',\n",
    "    112: 'left_mouth_2',\n",
    "    113: 'left_mouth_1',\n",
    "    114: 'left_mouth_5',\n",
    "    115: 'left_mouth_4',\n",
    "    116: 'mouth_bottom',\n",
    "    117: 'right_mouth_4',\n",
    "    118: 'right_mouth_5',\n",
    "    119: 'right_lip_1',\n",
    "    120: 'right_lip_2',\n",
    "    121: 'lip_top',\n",
    "    122: 'left_lip_2',\n",
    "    123: 'left_lip_1',\n",
    "    124: 'left_lip_3',\n",
    "    125: 'lip_bottom',\n",
    "    126: 'right_lip_3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21f24322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices shape: torch.Size([1016, 10475, 3])\n",
      "Joints shape: torch.Size([1016, 127, 3])\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def z_up_to_y_up(v):\n",
    "    v_matrix = R.from_rotvec(v)\n",
    "    z_to_y_rotation = R.from_euler('x', -90, degrees=True)\n",
    "    return (z_to_y_rotation * v_matrix).as_rotvec()\n",
    "\n",
    "global_orient = torch.from_numpy(z_up_to_y_up(poses[:, :3])).float()\n",
    "\n",
    "body_pose = torch.from_numpy(poses[:, 3:66]).float()\n",
    "jaw_pose = torch.from_numpy(poses[:, 66:69]).float()\n",
    "leye_pose = torch.from_numpy(poses[:, 69:72]).float()\n",
    "reye_pose = torch.from_numpy(poses[:, 72:75]).float()\n",
    "left_hand_pose = torch.from_numpy(poses[:, 75:120]).float()\n",
    "right_hand_pose = torch.from_numpy(poses[:, 120:165]).float()\n",
    "\n",
    "\n",
    "expressions_tensor = torch.from_numpy(expressions).float()\n",
    "betas_tensor = torch.from_numpy(betas[:10]).reshape(1, 10).float()\n",
    "transl_tensor = torch.from_numpy(transl).float()\n",
    "\n",
    "\n",
    "smplx_out = smplx_model(\n",
    "                    global_orient=global_orient,\n",
    "                    body_pose=body_pose,\n",
    "                    betas=betas_tensor,\n",
    "                    left_hand_pose=left_hand_pose,\n",
    "                    right_hand_pose=right_hand_pose,\n",
    "                    jaw_pose=jaw_pose,\n",
    "                    leye_pose=leye_pose,\n",
    "                    reye_pose=reye_pose,\n",
    "                    expression=expressions_tensor,\n",
    "                    transl=transl_tensor\n",
    "                )\n",
    "vertices = smplx_out.vertices\n",
    "joints = smplx_out.joints\n",
    "print(\"Vertices shape:\", vertices.shape)\n",
    "print(\"Joints shape:\", joints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75616c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "def render(vertices, faces, image_size=(800, 800)):\n",
    "    vertices = vertices[0].cpu().numpy()\n",
    "\n",
    "    # ---- Build mesh ----\n",
    "    mesh = trimesh.Trimesh(vertices=vertices,\n",
    "                           faces=faces,\n",
    "                           process=False)\n",
    "    pm = pyrender.Mesh.from_trimesh(mesh, smooth=True)\n",
    "\n",
    "    # ---- Scene ----\n",
    "    scene = pyrender.Scene(bg_color=np.zeros(4))\n",
    "    # Add mesh\n",
    "    scene.add(pm)\n",
    "    \n",
    "    # ---- Auto center ----\n",
    "    center = mesh.bounds.mean(axis=0)\n",
    "    extents = mesh.extents\n",
    "    radius = np.linalg.norm(extents) * 0.5\n",
    "\n",
    "    # ---- Camera ----\n",
    "    camera = pyrender.PerspectiveCamera(yfov=np.deg2rad(45.0))\n",
    "    scene.add(\n",
    "        camera,\n",
    "        pose=np.array([\n",
    "            [1, 0, 0, center[0]],\n",
    "            [0, 1, 0, center[1]],\n",
    "            [0, 0, 1, center[2] + 2.5 * radius],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ---- Light (simple directional) ----\n",
    "    light = pyrender.DirectionalLight(color=np.ones(3), intensity=3.0)\n",
    "    scene.add(\n",
    "        light,\n",
    "        pose=np.array([\n",
    "            [1, 0, 0, center[0]],\n",
    "            [0, 1, 0, center[1] + radius],\n",
    "            [0, 0, 1, center[2] + radius],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ---- Render ----\n",
    "    w, h = image_size\n",
    "    r = pyrender.OffscreenRenderer(w, h)\n",
    "    color, _ = r.render(scene)\n",
    "    r.delete()\n",
    "\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c65849eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in tqdm(range(300, 600, 3)):\n",
    "    img = render(vertices[i:i+1], smplx_model.faces)\n",
    "    frames.append(img)\n",
    "\n",
    "imageio.mimwrite('smplx_mocap.mp4', frames, fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
